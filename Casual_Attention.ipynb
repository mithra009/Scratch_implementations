{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d37a2d8c"
      },
      "source": [
        "## Implement Causal Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ea7fd67",
        "outputId": "5633781c-4a2c-4489-9352-f09de3a61c00"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CausalAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        # Initialize linear layers for query, key, and value transformations\n",
        "        self.query_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass x through the query, key, and value linear layers\n",
        "        query = self.query_proj(x)\n",
        "        key = self.key_proj(x)\n",
        "        value = self.value_proj(x)\n",
        "\n",
        "        # Calculate raw attention scores (dot product between query and transpose of key)\n",
        "        # Scaling by sqrt(embed_dim) to prevent very large values\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / (self.embed_dim ** 0.5)\n",
        "\n",
        "        # Create a causal mask (look-ahead mask)\n",
        "        seq_len = x.size(1) # Assuming input x shape is (batch_size, seq_len, embed_dim)\n",
        "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device) * float('-inf'), diagonal=1)\n",
        "\n",
        "        # Apply the causal mask to the attention scores\n",
        "        scores = scores + causal_mask\n",
        "\n",
        "        # Apply softmax to get attention probabilities\n",
        "        attention_probs = torch.softmax(scores, dim=-1)\n",
        "\n",
        "        # Apply dropout to attention probabilities\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # Multiply attention probabilities with the value tensor\n",
        "        output = torch.matmul(attention_probs, value)\n",
        "\n",
        "        return output\n",
        "\n",
        "print(\"CausalAttention class defined successfully.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CausalAttention class defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d9c0e33"
      },
      "source": [
        "## Test Causal Attention\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d2fa743",
        "outputId": "ff149cbc-3126-4974-88c0-ceb7097a3ef1"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 1. Define an embedding dimension, batch size, and sequence length\n",
        "embed_dim = 64\n",
        "batch_size = 2\n",
        "seq_len = 10\n",
        "\n",
        "# 2. Create a random input tensor x\n",
        "x = torch.randn(batch_size, seq_len, embed_dim)\n",
        "\n",
        "# 3. Instantiate the CausalAttention class\n",
        "causal_attention_model = CausalAttention(embed_dim)\n",
        "\n",
        "# 4. Pass the input tensor x through the CausalAttention instance\n",
        "output = causal_attention_model(x)\n",
        "\n",
        "# 5. Print the shape of the output tensor\n",
        "print(f\"Input tensor shape: {x.shape}\")\n",
        "print(f\"Output tensor shape: {output.shape}\")\n",
        "\n",
        "# Assert that the output shape is as expected\n",
        "assert output.shape == (batch_size, seq_len, embed_dim), f\"Expected output shape ({batch_size}, {seq_len}, {embed_dim}), but got {output.shape}\"\n",
        "print(\"Test passed: Output shape is correct.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tensor shape: torch.Size([2, 10, 64])\n",
            "Output tensor shape: torch.Size([2, 10, 64])\n",
            "Test passed: Output shape is correct.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce5f05a9"
      },
      "source": [
        "## Implement Causal Attention in TensorFlow\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2b2b5a8",
        "outputId": "b1ddfedf-82df-4945-c645-e9fb09a08312"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class CausalAttentionTF(layers.Layer):\n",
        "    def __init__(self, embed_dim, dropout_rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        # Initialize dense layers for query, key, and value transformations\n",
        "        self.query_proj = layers.Dense(embed_dim, use_bias=False)\n",
        "        self.key_proj = layers.Dense(embed_dim, use_bias=False)\n",
        "        self.value_proj = layers.Dense(embed_dim, use_bias=False)\n",
        "        self.dropout = layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Pass inputs through the query, key, and value dense layers\n",
        "        query = self.query_proj(inputs)\n",
        "        key = self.key_proj(inputs)\n",
        "        value = self.value_proj(inputs)\n",
        "\n",
        "        # Calculate raw attention scores (dot product between query and transpose of key)\n",
        "        # Scaling by sqrt(embed_dim) to prevent very large values\n",
        "        scores = tf.matmul(query, key, transpose_b=True) / tf.math.sqrt(tf.cast(self.embed_dim, tf.float32))\n",
        "        print(\"Scores\",scores)\n",
        "        # Create a causal mask (look-ahead mask)\n",
        "        seq_len = tf.shape(inputs)[1] # Assuming input shape is (batch_size, seq_len, embed_dim)\n",
        "        # Create a lower triangular mask and then invert it to get an upper triangular mask\n",
        "        # for positions to be masked (set to -inf)\n",
        "        causal_mask = tf.cast(tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0), tf.float32)\n",
        "        causal_mask = (1 - causal_mask) * -1e9 # Mask out future positions with a large negative number\n",
        "        print(\"Causal Mask\",causal_mask)\n",
        "        # Apply the causal mask to the attention scores\n",
        "        # The mask needs to be broadcastable to the scores shape (batch_size, seq_len, seq_len)\n",
        "        scores = scores + causal_mask\n",
        "\n",
        "        # Apply softmax to get attention probabilities\n",
        "        attention_probs = tf.nn.softmax(scores, axis=-1)\n",
        "\n",
        "        # Apply dropout to attention probabilities\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "        print(\"Attention Probs\",attention_probs)\n",
        "        # Multiply attention probabilities with the value tensor\n",
        "        output = tf.matmul(attention_probs, value)\n",
        "        print(\"Output\",output)\n",
        "        return output\n",
        "\n",
        "print(\"CausalAttentionTF class defined successfully.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CausalAttentionTF class defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78d3e8ce"
      },
      "source": [
        "## Test Causal Attention in TensorFlow\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7397e445",
        "outputId": "81557316-2173-4083-9412-44f34d4e2bf9"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 1. Define an embedding dimension, batch size, and sequence length\n",
        "embed_dim = 3\n",
        "batch_size = 1\n",
        "seq_len = 5\n",
        "\n",
        "# 2. Create a random input tensor x\n",
        "x = tf.random.normal((batch_size, seq_len, embed_dim))\n",
        "\n",
        "# 3. Instantiate the CausalAttentionTF class\n",
        "causal_attention_tf_model = CausalAttentionTF(embed_dim)\n",
        "\n",
        "# 4. Pass the input tensor x through the CausalAttentionTF instance\n",
        "output_tf = causal_attention_tf_model(x)\n",
        "\n",
        "# 5. Print the shape of the output tensor\n",
        "print(f\"Input tensor shape (TF): {x.shape}\")\n",
        "print(f\"Output tensor shape (TF): {output_tf.shape}\")\n",
        "\n",
        "# Assert that the output shape is as expected\n",
        "assert output_tf.shape == (batch_size, seq_len, embed_dim), f\"Expected output shape ({batch_size}, {seq_len}, {embed_dim}), but got {output_tf.shape}\"\n",
        "print(\"Test passed: TensorFlow output shape is correct.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scores Tensor(\"truediv:0\", shape=(1, 5, 5), dtype=float32)\n",
            "Causal Mask Tensor(\"mul:0\", shape=(5, 5), dtype=float32)\n",
            "Attention Probs Tensor(\"Softmax:0\", shape=(1, 5, 5), dtype=float32)\n",
            "Output Tensor(\"MatMul_1:0\", shape=(1, 5, 3), dtype=float32)\n",
            "Scores tf.Tensor(\n",
            "[[[-1.2593594   1.2487633  -0.3013413  -1.3586303   0.10356779]\n",
            "  [ 1.5379086  -0.2092953  -0.32554933 -0.10354766 -0.05004847]\n",
            "  [-0.539802   -0.2679707   0.2952769   0.48303255 -0.00340527]\n",
            "  [-0.8458488   0.22361538  0.11110591  0.0240486   0.04574059]\n",
            "  [ 0.21569961 -0.01592045 -0.05388021 -0.02061073 -0.00497768]]], shape=(1, 5, 5), dtype=float32)\n",
            "Causal Mask tf.Tensor(\n",
            "[[-0.e+00 -1.e+09 -1.e+09 -1.e+09 -1.e+09]\n",
            " [-0.e+00 -0.e+00 -1.e+09 -1.e+09 -1.e+09]\n",
            " [-0.e+00 -0.e+00 -0.e+00 -1.e+09 -1.e+09]\n",
            " [-0.e+00 -0.e+00 -0.e+00 -0.e+00 -1.e+09]\n",
            " [-0.e+00 -0.e+00 -0.e+00 -0.e+00 -0.e+00]], shape=(5, 5), dtype=float32)\n",
            "Attention Probs tf.Tensor(\n",
            "[[[1.         0.         0.         0.         0.        ]\n",
            "  [0.8515998  0.14840023 0.         0.         0.        ]\n",
            "  [0.2165739  0.28422412 0.49920198 0.         0.        ]\n",
            "  [0.11230605 0.3272394  0.29241747 0.26803705 0.        ]\n",
            "  [0.24105339 0.191215   0.18409255 0.19032022 0.19331889]]], shape=(1, 5, 5), dtype=float32)\n",
            "Output tf.Tensor(\n",
            "[[[-1.2321019e+00  1.4149300e+00  4.3538561e-01]\n",
            "  [-1.1938241e+00  1.1306946e+00  2.8002626e-01]\n",
            "  [-1.9207239e-02  1.4906292e-01  3.9178923e-02]\n",
            "  [-2.4477886e-01 -2.2846936e-01 -5.8516860e-05]\n",
            "  [-4.0254736e-01  5.2063182e-02  7.2498590e-02]]], shape=(1, 5, 3), dtype=float32)\n",
            "Input tensor shape (TF): (1, 5, 3)\n",
            "Output tensor shape (TF): (1, 5, 3)\n",
            "Test passed: TensorFlow output shape is correct.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ofgmY-DdCp_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}