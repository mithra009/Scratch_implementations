{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f919edf"
      },
      "source": [
        "## Setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af7970cd",
        "outputId": "615475a8-eb11-40bc-ec6c-f74f6ef245a5"
      },
      "source": [
        "import torch\n",
        "\n",
        "num_tokens = 5\n",
        "embed_dim = 8\n",
        "\n",
        "print(f\"Number of tokens: {num_tokens}\")\n",
        "print(f\"Embedding dimension: {embed_dim}\")\n",
        "\n",
        "Q = torch.randn(num_tokens, embed_dim)\n",
        "K = torch.randn(num_tokens, embed_dim)\n",
        "V = torch.randn(num_tokens, embed_dim)\n",
        "\n",
        "print(f\"\\nShape of Query (Q) matrix: {Q.shape}\")\n",
        "print(f\"Shape of Key (K) matrix: {K.shape}\")\n",
        "print(f\"Shape of Value (V) matrix: {V.shape}\")\n",
        "\n",
        "print(\"Dummy Q, K, V matrices created successfully.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens: 5\n",
            "Embedding dimension: 8\n",
            "\n",
            "Shape of Query (Q) matrix: torch.Size([5, 8])\n",
            "Shape of Key (K) matrix: torch.Size([5, 8])\n",
            "Shape of Value (V) matrix: torch.Size([5, 8])\n",
            "Dummy Q, K, V matrices created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eb3772f"
      },
      "source": [
        "## Compute PyTorch Attention Scores (Scaled)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f380b77",
        "outputId": "49d1c7db-e7d8-466e-8ce7-5a4e81fe6b8f"
      },
      "source": [
        "print(\"\\n--- Calculating PyTorch Scaled Attention Scores ---\")\n",
        "\n",
        "attention_scores = torch.matmul(Q, K.T)\n",
        "\n",
        "scaling_factor = torch.sqrt(torch.tensor(embed_dim, dtype=torch.float32))\n",
        "\n",
        "scaled_attention_scores = attention_scores / scaling_factor\n",
        "\n",
        "print(f\"Shape of scaled attention scores: {scaled_attention_scores.shape}\")\n",
        "\n",
        "print(\"Scaled attention scores (Q @ K.T / sqrt(embed_dim)):\\n\", scaled_attention_scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Calculating PyTorch Scaled Attention Scores ---\n",
            "Shape of scaled attention scores: torch.Size([5, 5])\n",
            "Scaled attention scores (Q @ K.T / sqrt(embed_dim)):\n",
            " tensor([[-0.9626, -1.0573, -2.0671, -0.7733,  1.0316],\n",
            "        [ 0.6510, -0.6209, -1.1043, -0.3764, -0.6955],\n",
            "        [-0.1537, -0.1260, -0.1983,  0.0680,  0.8189],\n",
            "        [ 0.6628,  0.5203,  2.0698,  0.9484,  1.6436],\n",
            "        [ 0.9044, -0.2830,  0.5115,  0.2024, -0.6457]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1964b365"
      },
      "source": [
        "## Compute Attention Weights (Scaled)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d6f2265",
        "outputId": "a59a2896-a02b-478a-91bf-38fd10267364"
      },
      "source": [
        "print(\"\\n--- Calculating PyTorch Scaled Attention Weights ---\")\n",
        "\n",
        "attention_weights = torch.softmax(scaled_attention_scores, dim=-1)\n",
        "\n",
        "print(f\"Shape of attention weights: {attention_weights.shape}\")\n",
        "\n",
        "print(\"Attention Weights (Softmax of Scaled Q @ K.T):\\n\", attention_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Calculating PyTorch Scaled Attention Weights ---\n",
            "Shape of attention weights: torch.Size([5, 5])\n",
            "Attention Weights (Softmax of Scaled Q @ K.T):\n",
            " tensor([[0.0926, 0.0843, 0.0307, 0.1119, 0.6805],\n",
            "        [0.4828, 0.1353, 0.0835, 0.1728, 0.1256],\n",
            "        [0.1454, 0.1495, 0.1391, 0.1815, 0.3846],\n",
            "        [0.1005, 0.0872, 0.4105, 0.1337, 0.2681],\n",
            "        [0.3720, 0.1135, 0.2512, 0.1844, 0.0790]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82594fe2"
      },
      "source": [
        "## Compute  Context Vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5181c477",
        "outputId": "67a08aba-d9ef-4925-e524-be198aa1070a"
      },
      "source": [
        "print(\"\\n--- Calculating PyTorch Scaled Context Vector ---\")\n",
        "\n",
        "context_vector = torch.matmul(attention_weights, V)\n",
        "\n",
        "print(f\"Shape of context vector: {context_vector.shape}\")\n",
        "\n",
        "print(\"Context Vector (Attention Weights @ V):\\n\", context_vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Calculating PyTorch Scaled Context Vector ---\n",
            "Shape of context vector: torch.Size([5, 8])\n",
            "Context Vector (Attention Weights @ V):\n",
            " tensor([[-0.1018, -0.6984, -0.2798,  0.8216, -0.2479,  0.0859, -0.2881,  0.6673],\n",
            "        [ 0.3728, -1.0027,  0.1836,  0.0792,  0.0076, -0.7703, -0.9220, -0.0298],\n",
            "        [ 0.1161, -0.6166, -0.2179,  0.4638,  0.0122, -0.1560, -0.6473,  0.2247],\n",
            "        [ 0.3370, -0.6782,  0.0848,  0.2377,  0.1385, -0.5586, -0.7788, -0.3333],\n",
            "        [ 0.4861, -0.9184,  0.2096, -0.0226,  0.1405, -0.8342, -0.9711, -0.3166]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a927e82",
        "outputId": "fa87ed96-50a2-4c99-919e-23810b6b10d7"
      },
      "source": [
        "print(\"\\n--- Initial Q, K, V Matrices (PyTorch) ---\")\n",
        "print(\"Original Query (Q) matrix representing token representations for queries:\\n\", Q)\n",
        "print(\"\\nOriginal Key (K) matrix representing token representations for keys:\\n\", K)\n",
        "print(\"\\nOriginal Value (V) matrix representing token representations for values:\\n\", V)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Initial Q, K, V Matrices (PyTorch) ---\n",
            "Original Query (Q) matrix representing token representations for queries:\n",
            " tensor([[ 0.6977,  0.4826, -1.8565,  1.3198,  0.3462,  0.7798, -0.4995, -0.9835],\n",
            "        [ 0.2701,  1.9504,  0.2245,  1.2016, -0.1669, -0.3142,  0.5633, -0.3367],\n",
            "        [ 0.0395, -0.3411, -0.5176, -1.8177,  1.0107,  0.4173,  1.4420, -1.3674],\n",
            "        [ 1.1321,  0.5711,  0.1820, -1.7744, -0.5664, -0.2617, -0.5426, -0.6553],\n",
            "        [-1.0340,  0.3379,  1.4092, -0.3155,  0.0545, -0.9417,  0.2377, -0.0266]])\n",
            "\n",
            "Original Key (K) matrix representing token representations for keys:\n",
            " tensor([[ 0.7875,  0.5182,  1.2047, -0.1642,  0.2804, -1.4729,  0.1833, -0.0753],\n",
            "        [ 1.6376, -1.3338,  0.2757, -0.3444, -1.3397, -0.7225,  1.1119,  0.9560],\n",
            "        [ 0.1045,  0.1098,  0.3895, -2.5926, -2.0610, -0.5188, -0.8200,  1.1382],\n",
            "        [ 1.0796, -0.2785,  0.7774, -0.7364,  0.1170, -0.5735, -0.3661,  0.1700],\n",
            "        [ 0.1256,  0.3077, -0.4248, -1.3286, -0.2098,  1.2452, -2.0347, -1.7609]])\n",
            "\n",
            "Original Value (V) matrix representing token representations for values:\n",
            " tensor([[ 0.6050, -1.8258,  1.0399, -0.2072, -0.3134, -1.7996, -1.0228, -0.2193],\n",
            "        [-1.3290,  1.1666, -0.9286,  1.2985,  0.2107,  0.9297, -1.7376,  0.2938],\n",
            "        [ 0.7136, -0.7173,  0.7368, -0.1852,  0.4034, -1.3964, -1.1269, -1.5523],\n",
            "        [ 1.4030, -0.7064, -1.2825, -0.7694,  0.9379,  0.3231, -0.6356,  0.2284],\n",
            "        [-0.3303, -0.7737, -0.2601,  1.2097, -0.5202,  0.2659,  0.0863,  1.0065]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69bbcc22",
        "outputId": "72a59505-806e-4659-f1f2-6ea68117d531"
      },
      "source": [
        "print(\"\\n--- Scaled Attention Scores (PyTorch) ---\")\n",
        "print(\"Raw attention scores (Q @ K.T / sqrt(embed_dim)) showing scaled similarity between query and key tokens:\\n\",\n",
        "scaled_attention_scores,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Scaled Attention Scores (PyTorch) ---\n",
            "Raw attention scores (Q @ K.T / sqrt(embed_dim)) showing scaled similarity between query and key tokens:\n",
            " tensor([[-0.9626, -1.0573, -2.0671, -0.7733,  1.0316],\n",
            "        [ 0.6510, -0.6209, -1.1043, -0.3764, -0.6955],\n",
            "        [-0.1537, -0.1260, -0.1983,  0.0680,  0.8189],\n",
            "        [ 0.6628,  0.5203,  2.0698,  0.9484,  1.6436],\n",
            "        [ 0.9044, -0.2830,  0.5115,  0.2024, -0.6457]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fbfaf95",
        "outputId": "a59bdc29-2eba-42d7-8d96-97a1355c2169"
      },
      "source": [
        "print(\"\\n--- Scaled Attention Weights (PyTorch) ---\")\n",
        "print(\"Normalized attention weights (Softmax of Scaled Q @ K.T) showing how much each query token attends to each key token:\\n\",\n",
        "attention_weights,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Scaled Attention Weights (PyTorch) ---\n",
            "Normalized attention weights (Softmax of Scaled Q @ K.T) showing how much each query token attends to each key token:\n",
            " tensor([[0.0926, 0.0843, 0.0307, 0.1119, 0.6805],\n",
            "        [0.4828, 0.1353, 0.0835, 0.1728, 0.1256],\n",
            "        [0.1454, 0.1495, 0.1391, 0.1815, 0.3846],\n",
            "        [0.1005, 0.0872, 0.4105, 0.1337, 0.2681],\n",
            "        [0.3720, 0.1135, 0.2512, 0.1844, 0.0790]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a29dc1f",
        "outputId": "3a566764-e22a-4db7-f65f-1507df652256"
      },
      "source": [
        "print(\"\\n--- Scaled Context Vector (PyTorch) ---\")\n",
        "print(\"Final context vector (Attention Weights @ V) representing the context-aware representation for each token:\\n\",\n",
        "context_vector,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Scaled Context Vector (PyTorch) ---\n",
            "Final context vector (Attention Weights @ V) representing the context-aware representation for each token:\n",
            " tensor([[-0.1018, -0.6984, -0.2798,  0.8216, -0.2479,  0.0859, -0.2881,  0.6673],\n",
            "        [ 0.3728, -1.0027,  0.1836,  0.0792,  0.0076, -0.7703, -0.9220, -0.0298],\n",
            "        [ 0.1161, -0.6166, -0.2179,  0.4638,  0.0122, -0.1560, -0.6473,  0.2247],\n",
            "        [ 0.3370, -0.6782,  0.0848,  0.2377,  0.1385, -0.5586, -0.7788, -0.3333],\n",
            "        [ 0.4861, -0.9184,  0.2096, -0.0226,  0.1405, -0.8342, -0.9711, -0.3166]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9a458e9",
        "outputId": "47a247ba-6c22-4304-8804-5d8f6e9cc4b9"
      },
      "source": [
        "print(\"\\n--- Calculating TensorFlow Scaled Attention Scores ---\")\n",
        "\n",
        "attention_scores_tf_raw = tf.matmul(Q_tf, tf.transpose(K_tf))\n",
        "\n",
        "scaling_factor_tf = tf.sqrt(tf.cast(embed_dim, tf.float32))\n",
        "\n",
        "scaled_attention_scores_tf = attention_scores_tf_raw / scaling_factor_tf\n",
        "\n",
        "print(f\"Shape of scaled attention scores (TensorFlow): {scaled_attention_scores_tf.shape}\")\n",
        "\n",
        "print(\"Scaled attention scores (Q_tf @ K_tf.T / sqrt(embed_dim)):\\n\", scaled_attention_scores_tf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Calculating TensorFlow Scaled Attention Scores ---\n",
            "Shape of scaled attention scores (TensorFlow): (5, 5)\n",
            "Scaled attention scores (Q_tf @ K_tf.T / sqrt(embed_dim)):\n",
            " tf.Tensor(\n",
            "[[-1.1715143  -0.70157117  1.8508699   0.7938507  -0.47439685]\n",
            " [-0.9528719  -0.04579314  1.5392618  -0.48467758  0.74428874]\n",
            " [-0.294289    1.1593198   0.17506891  0.61089295  0.10231236]\n",
            " [-0.9951796  -0.66143984  0.21328577 -0.67285043 -0.04337731]\n",
            " [ 0.19320318 -0.7317817  -2.87341    -0.5238799  -0.87627673]], shape=(5, 5), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acf673d1",
        "outputId": "dda1785e-0968-4dd9-c540-238299c9c81c"
      },
      "source": [
        "print(\"\\n--- Calculating TensorFlow Scaled Attention Weights ---\")\n",
        "\n",
        "attention_weights_tf = tf.nn.softmax(scaled_attention_scores_tf, axis=-1)\n",
        "\n",
        "print(f\"Shape of attention weights (TensorFlow): {attention_weights_tf.shape}\")\n",
        "\n",
        "print(\"Attention Weights (Softmax of Scaled Q_tf @ K_tf.T):\\n\", attention_weights_tf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Calculating TensorFlow Scaled Attention Weights ---\n",
            "Shape of attention weights (TensorFlow): (5, 5)\n",
            "Attention Weights (Softmax of Scaled Q_tf @ K_tf.T):\n",
            " tf.Tensor(\n",
            "[[0.03097358 0.04955472 0.63620365 0.22107443 0.06219358]\n",
            " [0.04420935 0.1095099  0.5343601  0.0706071  0.2413135 ]\n",
            " [0.09227954 0.39482048 0.14755195 0.22815023 0.1371978 ]\n",
            " [0.10293112 0.14371035 0.3446486  0.14207985 0.26663008]\n",
            " [0.4396615  0.17434223 0.02047884 0.21463138 0.15088609]], shape=(5, 5), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3d574d9",
        "outputId": "1da22a17-5c52-43d4-995f-77aae4ae60a2"
      },
      "source": [
        "print(\"\\n--- Calculating TensorFlow Scaled Context Vector ---\")\n",
        "\n",
        "context_vector_tf = tf.matmul(attention_weights_tf, V_tf)\n",
        "\n",
        "print(f\"Shape of context vector (TensorFlow): {context_vector_tf.shape}\")\n",
        "\n",
        "print(\"Context Vector (Attention Weights @ V_tf):\\n\", context_vector_tf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Calculating TensorFlow Scaled Context Vector ---\n",
            "Shape of context vector (TensorFlow): (5, 8)\n",
            "Context Vector (Attention Weights @ V_tf):\n",
            " tf.Tensor(\n",
            "[[ 1.1241323  -0.59088886 -0.19652964  0.21491177  0.48043168 -0.08062097\n",
            "  -0.971363   -0.36646673]\n",
            " [ 1.06236     0.03360325 -0.40229282  0.5087005   0.21427517 -0.14786546\n",
            "  -0.7432229  -0.5621588 ]\n",
            " [ 0.27724195 -0.03608703 -1.2358981   0.63904124 -0.45489448 -0.14722407\n",
            "  -0.02529149 -0.52401996]\n",
            " [ 0.6535382  -0.0276138  -0.4779798   0.2838771  -0.24898633 -0.11048048\n",
            "  -0.5758577  -0.46360675]\n",
            " [ 0.00838823 -0.5870342  -0.5820206  -0.55757934 -0.8990232   0.41084766\n",
            "  -0.362198    0.06709458]], shape=(5, 8), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49c57ca8",
        "outputId": "0b7cd855-93c1-456c-f839-59d16e058621"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(\"\\n--- Initial Q, K, V Matrices (TensorFlow) ---\")\n",
        "print(\"Original Query (Q_tf) matrix representing token representations for queries:\\n\", Q_tf)\n",
        "print(\"\\nOriginal Key (K_tf) matrix representing token representations for keys:\\n\", K_tf)\n",
        "print(\"\\nOriginal Value (V_tf) matrix representing token representations for values:\\n\", V_tf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Initial Q, K, V Matrices (TensorFlow) ---\n",
            "Original Query (Q_tf) matrix representing token representations for queries:\n",
            " tf.Tensor(\n",
            "[[-0.9146993  -1.2328101   0.27749616 -1.0733912   0.4440446  -1.3729824\n",
            "   1.0591654   2.1605346 ]\n",
            " [-1.7342167  -2.11384    -0.03782409 -1.4539984   0.49261218  0.05395482\n",
            "  -1.1483284   0.6430254 ]\n",
            " [-1.6785321   0.09654163  1.2842544  -1.1607221  -0.41235134 -1.1613528\n",
            "   1.0900464  -0.91839594]\n",
            " [-1.1268284   0.15216608  0.84470975 -0.7586678  -0.34276998  2.2665145\n",
            "  -0.28306943  0.3309613 ]\n",
            " [ 1.9150887   0.09954559  1.2366453   0.18552396 -1.3960669   0.58268857\n",
            "   1.0396348  -0.27854902]], shape=(5, 8), dtype=float32)\n",
            "\n",
            "Original Key (K_tf) matrix representing token representations for keys:\n",
            " tf.Tensor(\n",
            "[[ 0.26634452  0.3253534  -0.828123    0.7098225  -1.1356668  -0.7047835\n",
            "  -0.47131383 -0.7596592 ]\n",
            " [-0.59167933  0.8698879  -0.93958133 -0.9433399  -0.8200248  -1.1388083\n",
            "  -0.51563406 -1.3229511 ]\n",
            " [-2.2696419  -0.44619924 -0.57590055  1.089064    2.7966573   0.13700458\n",
            "   0.82486844  0.93047625]\n",
            " [-0.39046794  0.37065384  0.4721116   0.62198484  1.4422551  -0.70720696\n",
            "   0.95158815  0.12147941]\n",
            " [-0.3449197   0.13430128 -0.8821502  -0.8055434  -0.29769132 -0.25634727\n",
            "  -0.9843214  -0.59652346]], shape=(5, 8), dtype=float32)\n",
            "\n",
            "Original Value (V_tf) matrix representing token representations for values:\n",
            " tf.Tensor(\n",
            "[[ 0.13825569 -1.2560441  -0.1848242  -1.7339451  -0.82840234  1.4166547\n",
            "  -0.6703198   0.8374197 ]\n",
            " [ 0.34019288  0.9666465  -3.1886322   2.4173045  -0.13994    -0.3150235\n",
            "   1.2132641  -1.1357107 ]\n",
            " [ 1.9863183  -0.41992703 -0.13798003  0.64540297  1.4338282  -0.00867123\n",
            "  -1.2508308  -0.50977004]\n",
            " [-0.7382771  -1.9038607   0.23447052 -1.2595378  -1.3324733  -0.29183683\n",
            "  -0.83326226  0.19893497]\n",
            " [ 0.04024545  1.4176294   0.05072524  0.26808706 -1.6819423  -0.6247365\n",
            "  -0.49406543 -0.89698446]], shape=(5, 8), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "196f5514",
        "outputId": "569534c8-ab3e-45b1-82db-31739e283441"
      },
      "source": [
        "print(\"\\n--- Scaled Attention Scores (TensorFlow) ---\")\n",
        "print(\"Raw attention scores (Q_tf @ K_tf.T / sqrt(embed_dim)) showing scaled similarity between query and key tokens:\\n\",\n",
        "scaled_attention_scores_tf,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Scaled Attention Scores (TensorFlow) ---\n",
            "Raw attention scores (Q_tf @ K_tf.T / sqrt(embed_dim)) showing scaled similarity between query and key tokens:\n",
            " tf.Tensor(\n",
            "[[-1.1715143  -0.70157117  1.8508699   0.7938507  -0.47439685]\n",
            " [-0.9528719  -0.04579314  1.5392618  -0.48467758  0.74428874]\n",
            " [-0.294289    1.1593198   0.17506891  0.61089295  0.10231236]\n",
            " [-0.9951796  -0.66143984  0.21328577 -0.67285043 -0.04337731]\n",
            " [ 0.19320318 -0.7317817  -2.87341    -0.5238799  -0.87627673]], shape=(5, 5), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c956fb9",
        "outputId": "fb56b932-4832-47aa-c5f9-9300e16937b1"
      },
      "source": [
        "print(\"\\n--- Scaled Attention Weights (TensorFlow) ---\")\n",
        "print(\"Normalized attention weights (Softmax of Scaled Q_tf @ K_tf.T) showing how much each query token attends to each key token:\\n\",\n",
        "attention_weights_tf,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Scaled Attention Weights (TensorFlow) ---\n",
            "Normalized attention weights (Softmax of Scaled Q_tf @ K_tf.T) showing how much each query token attends to each key token:\n",
            " tf.Tensor(\n",
            "[[0.03097358 0.04955472 0.63620365 0.22107443 0.06219358]\n",
            " [0.04420935 0.1095099  0.5343601  0.0706071  0.2413135 ]\n",
            " [0.09227954 0.39482048 0.14755195 0.22815023 0.1371978 ]\n",
            " [0.10293112 0.14371035 0.3446486  0.14207985 0.26663008]\n",
            " [0.4396615  0.17434223 0.02047884 0.21463138 0.15088609]], shape=(5, 5), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54ea2df3",
        "outputId": "bd61843a-7219-48b7-9242-21367bd00333"
      },
      "source": [
        "print(\"\\n--- Scaled Context Vector (TensorFlow) ---\")\n",
        "print(\"Final context vector (Attention Weights @ V_tf) representing the context-aware representation for each token:\\n\",\n",
        "context_vector_tf,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Scaled Context Vector (TensorFlow) ---\n",
            "Final context vector (Attention Weights @ V_tf) representing the context-aware representation for each token:\n",
            " tf.Tensor(\n",
            "[[ 1.1241323  -0.59088886 -0.19652964  0.21491177  0.48043168 -0.08062097\n",
            "  -0.971363   -0.36646673]\n",
            " [ 1.06236     0.03360325 -0.40229282  0.5087005   0.21427517 -0.14786546\n",
            "  -0.7432229  -0.5621588 ]\n",
            " [ 0.27724195 -0.03608703 -1.2358981   0.63904124 -0.45489448 -0.14722407\n",
            "  -0.02529149 -0.52401996]\n",
            " [ 0.6535382  -0.0276138  -0.4779798   0.2838771  -0.24898633 -0.11048048\n",
            "  -0.5758577  -0.46360675]\n",
            " [ 0.00838823 -0.5870342  -0.5820206  -0.55757934 -0.8990232   0.41084766\n",
            "  -0.362198    0.06709458]], shape=(5, 8), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1209c46e",
        "outputId": "157593ad-160a-406b-852c-75809b2acfd3"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "num_tokens = 5\n",
        "embed_dim = 8\n",
        "\n",
        "print(f\"Number of tokens: {num_tokens}\")\n",
        "print(f\"Embedding dimension: {embed_dim}\")\n",
        "\n",
        "Q_tf = tf.random.normal((num_tokens, embed_dim))\n",
        "K_tf = tf.random.normal((num_tokens, embed_dim))\n",
        "V_tf = tf.random.normal((num_tokens, embed_dim))\n",
        "\n",
        "print(f\"\\nShape of Query (Q_tf) matrix: {Q_tf.shape}\")\n",
        "print(f\"Shape of Key (K_tf) matrix: {K_tf.shape}\")\n",
        "print(f\"Shape of Value (V_tf) matrix: {V_tf.shape}\")\n",
        "\n",
        "print(\"Dummy Q_tf, K_tf, V_tf matrices created successfully.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens: 5\n",
            "Embedding dimension: 8\n",
            "\n",
            "Shape of Query (Q_tf) matrix: (5, 8)\n",
            "Shape of Key (K_tf) matrix: (5, 8)\n",
            "Shape of Value (V_tf) matrix: (5, 8)\n",
            "Dummy Q_tf, K_tf, V_tf matrices created successfully.\n"
          ]
        }
      ]
    }
  ]
}